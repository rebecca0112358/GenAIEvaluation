{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install Required Packages"
      ],
      "metadata": {
        "id": "1XtrTFDGeTC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install bert-score\n",
        "!pip install nltk\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Fik0zGMeG7WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpAK1uDVGtnf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import jaccard_score\n",
        "from rouge_score import rouge_scorer\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "from difflib import SequenceMatcher\n",
        "import numpy as np\n",
        "from docx import Document\n",
        "from bert_score import score as bert_score\n",
        "#from moverscore import word_mover_score\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "c3pPl5deecYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example: Inclusion/Exclusion Criteria\n",
        "Ground truth is pulled from protocolv1.docx\n",
        "\n",
        "Generated text is created using ChatGPT"
      ],
      "metadata": {
        "id": "OOkrZw8Peypb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ground truth and generated criteria (combined into one string each for comparison)\n",
        "ground_truth = \"\"\"\n",
        "Inclusion Criteria:\n",
        "- Adults aged 18 to 75 years\n",
        "- Diagnosis of SLE as per ACR/EULAR 2019 classification criteria\n",
        "- SLEDAI-2K score â‰¥6 at screening\n",
        "- Positive for ANA (antinuclear antibodies) or anti-dsDNA at screening\n",
        "- Receiving stable background therapy for SLE, including corticosteroids\n",
        "  (â‰¤10 mg/day prednisone or equivalent), antimalarials, and/or immunosuppressants\n",
        "  for â‰¥12 weeks\n",
        "- Willing and able to provide informed consent and comply with study procedures\n",
        "\n",
        "Exclusion Criteria:\n",
        "- Active severe lupus nephritis or CNS lupus\n",
        "- History of severe allergic reactions to monoclonal antibodies\n",
        "- Active or chronic infections, including tuberculosis, hepatitis B or C, HIV\n",
        "- Use of biologic therapy within 12 weeks of screening\n",
        "- Pregnancy or breastfeeding\n",
        "- Any other medical condition that, in the investigatorâ€™s opinion, would\n",
        "  compromise patient safety or data integrity\n",
        "\"\"\"\n",
        "\n",
        "generated = \"\"\"\n",
        "Inclusion Criteria:\n",
        "- Age between 18 and 75 years\n",
        "- Confirmed diagnosis of Systemic Lupus Erythematosus (SLE) according to\n",
        "  standard criteria\n",
        "- Moderate to severe disease activity, with SLEDAI-2K score of at least 6\n",
        "- Positive test for antinuclear antibodies (ANA) or anti-dsDNA\n",
        "- On stable treatment regimen for lupus for at least 12 weeks\n",
        "- Ability to provide informed consent\n",
        "\n",
        "Exclusion Criteria:\n",
        "- Active lupus affecting the kidneys or central nervous system\n",
        "- History of allergic reactions to antibody-based therapies\n",
        "- Ongoing infections such as tuberculosis, hepatitis B/C, or HIV\n",
        "- Recent use of biologic treatments (within last 3 months)\n",
        "- Pregnant or nursing women\n",
        "- Any medical issue that could pose risk or affect study validity\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "llHMYAIzGybm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate ROUGE Score"
      ],
      "metadata": {
        "id": "FpdWGl8RfR13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE Score\n",
        "rouge = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = rouge.score(ground_truth, generated)"
      ],
      "metadata": {
        "id": "m5ZajofFHB6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rouge_scores)"
      ],
      "metadata": {
        "id": "Q3ZCql0mHGVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate METEOR Score"
      ],
      "metadata": {
        "id": "AU4isZyOfbB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize both reference and hypothesis\n",
        "reference = [word_tokenize(ground_truth)]\n",
        "hypothesis = word_tokenize(generated)\n",
        "\n",
        "# Compute METEOR score\n",
        "score = meteor_score(reference, hypothesis)\n",
        "print(f\"METEOR Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "ySqUjHAdJCOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate JSD\n",
        "0 to 1 where 0 is identicle"
      ],
      "metadata": {
        "id": "tbBh7hisIU9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jensen-Shannon Divergence\n",
        "vectorizer = CountVectorizer().fit([ground_truth, generated])\n",
        "X = vectorizer.transform([ground_truth, generated]).toarray()\n",
        "jsd = jensenshannon(X[0], X[1])"
      ],
      "metadata": {
        "id": "rxGRIrkdHW2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(jsd)"
      ],
      "metadata": {
        "id": "otKXCr5LIAjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate Levenshtein Similarity Ratio\n",
        "0 to 1 where 0 is completely different and 1 is identicle"
      ],
      "metadata": {
        "id": "jrFCYt5IIZ4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Levenshtein Similarity Ratio\n",
        "lev_ratio = SequenceMatcher(None, ground_truth, generated).ratio()"
      ],
      "metadata": {
        "id": "Fyy5CseTHayi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lev_ratio)"
      ],
      "metadata": {
        "id": "LT6_f-vVHbrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Score"
      ],
      "metadata": {
        "id": "9tU62B2FfvjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Revisiting inclusion/exclusion example"
      ],
      "metadata": {
        "id": "bkeuLJFrf462"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "references = [ground_truth]\n",
        "candidates = [generated]\n",
        "\n",
        "# BERTScore\n",
        "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=True)\n",
        "print(f\"BERTScore - Precision: {P.item():.4f}, Recall: {R.item():.4f}, F1: {F1.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "Y88uDyDU41CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Bert Score between generated informed consent document (icdv1) and gold standard (Vanderbilt)"
      ],
      "metadata": {
        "id": "RNwrqlPnf9Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to extract text from a DOCX file\n",
        "def read_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return text.strip()\n",
        "\n",
        "# Load the documents\n",
        "doc1_path = \"VanderbiltICD_SLE_Cognitive.docx\"\n",
        "doc2_path = \"icdv1.docx\"\n",
        "\n",
        "doc1_text = read_docx(doc1_path)\n",
        "doc2_text = read_docx(doc2_path)\n",
        "\n",
        "# Prepare for BERTScore\n",
        "candidates = [doc1_text]   # usually the generated or predicted text\n",
        "references = [doc2_text]   # usually the ground truth\n",
        "\n",
        "# Compute BERTScore\n",
        "P, R, F1 = bert_score(candidates, references, lang=\"en\", verbose=True)\n",
        "\n",
        "print(f\"\\nBERTScore Results:\")\n",
        "print(f\"  Precision: {P.item():.4f}\")\n",
        "print(f\"  Recall:    {R.item():.4f}\")\n",
        "print(f\"  F1 Score:  {F1.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "GnP_kMuO6nTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROUGE-L"
      ],
      "metadata": {
        "id": "o3thABJmOQ29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "background_summary = \"\"\"\n",
        "Systemic Lupus Erythematosus (SLE) is a chronic autoimmune disease marked by systemic inflammation and multi-organ involvement. While current treatmentsâ€”such as corticosteroids, antimalarials,\n",
        "and immunosuppressantsâ€”offer some benefit, there remains a significant unmet need for safer, more effective therapies. Ilizomab is a novel monoclonal antibody targeting [specific pathway], which\n",
        "has shown promise in preclinical models by reducing inflammatory cytokines and autoantibody production.\n",
        "\n",
        "This Phase 2, multicenter, randomized, double-blind, placebo-controlled trial will evaluate the safety, efficacy, and pharmacokinetics of Ilizomab in approximately 150 adult patients with\n",
        "moderate to severe SLE. Participants will be randomized in a 2:1 ratio to receive Ilizomab or placebo over a 24-week treatment period, followed by 12 weeks of post-treatment follow-up.\n",
        "\n",
        "The primary endpoint is the proportion of patients achieving an SRI-4 response at Week 24. Secondary endpoints include changes in SLEDAI-2K scores, tapering of corticosteroids, biomarker\n",
        "trends (e.g., anti-dsDNA, complement levels), and patient-reported outcomes. Safety assessments include adverse event monitoring, immunogenicity evaluations, and lab testing.\n",
        "\n",
        "Eligibility criteria require adult patients aged 18â€“75 with confirmed SLE per ACR/EULAR 2019 criteria and SLEDAI-2K â‰¥6. Exclusion criteria include severe lupus nephritis or CNS involvement,\n",
        "active infections, recent biologic therapy, or pregnancy.\n",
        "\"\"\"\n",
        "\n",
        "# Load and clean text from docx\n",
        "def read_docx(path):\n",
        "    doc = Document(path)\n",
        "    return \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "# Load the full protocol as source\n",
        "source_text = read_docx(\"protocolv1.docx\")\n",
        "# Initialize ROUGE scorer (use stemming to reduce surface variation impact)\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(source_text, background_summary)\n",
        "\n",
        "# Display results\n",
        "print(\"ðŸ” ROUGE Evaluation (using protocol as reference)\")\n",
        "for metric, score in scores.items():\n",
        "    print(f\"{metric.upper()} â€” Precision: {score.precision:.4f}, Recall: {score.recall:.4f}, F1: {score.fmeasure:.4f}\")\n"
      ],
      "metadata": {
        "id": "wNkIhgguJT_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entailment Metrics"
      ],
      "metadata": {
        "id": "j5NoRDHLgT-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install required libraries\n",
        "\n",
        "Note: May require restarting notebook after install"
      ],
      "metadata": {
        "id": "H0gV0RFcgXnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/tingofurro/summac.git\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PuIUoPd8Mucz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece nltk transformers\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pSdMv5OvNErx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r-06KRVkN6x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_docx(path):\n",
        "    doc = Document(path)\n",
        "    return \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "# Load protocol and SAP documents\n",
        "protocol_text = read_docx(\"protocolv1.docx\")\n",
        "sap_text = read_docx(\"sapv1.docx\")\n"
      ],
      "metadata": {
        "id": "aO57CuU4NKDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from summac.model_summac import SummaCConv\n",
        "import torch\n",
        "\n",
        "#protocol_text = 'this is also a test'\n",
        "#sap_text = 'this is a test'\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate the model\n",
        "model = SummaCConv(models=[\"vitc\"], granularity=\"sentence\", device=device)\n",
        "\n",
        "# Score the SAP vs protocol\n",
        "results = model.score([protocol_text], [sap_text])\n",
        "\n",
        "# Show the overall entailment score\n",
        "#print(f\"SummaC Consistency Score: {results[0]['score']:.4f}\")\n",
        "print(f\"SummaC Consistency Score: {results['scores']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Nr8G4w_nNMtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "id": "N2jQ3ZIA_FIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source = checklist\n",
        "\n",
        "hypothesis = protocol\n",
        "\n",
        "generate question from source => is_answer = False"
      ],
      "metadata": {
        "id": "C6vDe4N9YGyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install DeepEval"
      ],
      "metadata": {
        "id": "9kRMlCaChTcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepeval"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sJXnXWBtxZgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set OpenAI API Key (Or other model)"
      ],
      "metadata": {
        "id": "VjC16M0DhZui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "IgtxhqFgDihM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Cell"
      ],
      "metadata": {
        "id": "pO9tadhDhpOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install posthog"
      ],
      "metadata": {
        "id": "d9LvoxKdkA5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import BiasMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.evaluate import evaluate\n",
        "\n",
        "\"\"\"\n",
        "# Setup test case\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Bias evaluation\",  # Not needed for bias detection\n",
        "    actual_output=doc_text,\n",
        "    expected_output=\"A document free of gender or socioeconomic bias\"\n",
        ")\n",
        "\n",
        "# Define the metric\n",
        "bias_metric = BiasMetric()\n",
        "\n",
        "# Run evaluation\n",
        "evaluate([test_case], [bias_metric])\n",
        "\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "PE7qxyn-8fdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unbiased Example"
      ],
      "metadata": {
        "id": "lUhU1lSphuc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Document to evaluate\n",
        "doc_text = \"\"\"\n",
        "All participants will be required to provide written informed consent prior to\n",
        "any study-related procedures. The informed consent process will be conducted by\n",
        "qualified study personnel in a private setting, allowing sufficient time for\n",
        "participants to review the consent document, ask questions, and discuss\n",
        "participation with family members or other advisors if desired.\n",
        "\n",
        "The consent form will describe the purpose of the study, the procedures involved,\n",
        "the duration of participation, potential risks and benefits, confidentiality of\n",
        "personal health information, and the voluntary nature of participation.\n",
        "Participants will be informed that they may withdraw from the study at any time\n",
        "without penalty or loss of benefits to which they are otherwise entitled.\n",
        "\n",
        "Special attention will be given to ensuring comprehension among participants\n",
        "with limited literacy or non-native English speakers. Translated consent forms\n",
        "will be provided when necessary, and interpreters will be available upon request.\n",
        "Additional safeguards will be applied for individuals with impaired\n",
        "decision-making capacity in accordance with institutional and regulatory guidelines.\n",
        "\n",
        "Documentation of informed consent will be maintained in each participantâ€™s\n",
        "study file. A copy of the signed consent form will be provided to the participant.\n",
        "\"\"\"\n",
        "\n",
        "# Setup test case\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Bias evaluation\",  # Not needed for bias detection\n",
        "    actual_output=doc_text,\n",
        "    expected_output=\"A document free of gender or socioeconomic bias\"\n",
        ")\n",
        "\n",
        "# Define the metric\n",
        "bias_metric = BiasMetric()\n",
        "\n",
        "# Run evaluation\n",
        "evaluate([test_case], [bias_metric])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I7tDLX52Rum3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Biased Example"
      ],
      "metadata": {
        "id": "syBxEf3rh0MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Document to evaluate\n",
        "doc_text = \"\"\"\n",
        "Eligible participants will be provided with a consent form to review and sign\n",
        "prior to enrollment. The form will explain the purpose of the study, procedures\n",
        "involved, and potential risks and benefits. As most participants are expected\n",
        "to be working adults with access to transportation and childcare, the study\n",
        "visits are scheduled during weekday business hours and cannot be adjusted.\n",
        "\n",
        "Female participants of childbearing age will be required to undergo pregnancy\n",
        "testing and agree to use approved forms of contraception throughout the study.\n",
        "Male participants will not be required to modify their behavior, as their\n",
        "involvement presents minimal reproductive risk.\n",
        "\n",
        "Compensation will be provided in the form of direct deposit only. Participants\n",
        "should have a checking account in order to receive payment. Additional resources\n",
        "or assistance with financial access will not be offered.\n",
        "\n",
        "Participants are expected to complete all study visits as outlined. Failure to\n",
        "do so may result in exclusion from the study or withholding of compensation.\n",
        "\"\"\"\n",
        "\n",
        "# Setup test case\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Bias evaluation\",  # Not needed for bias detection\n",
        "    actual_output=doc_text,\n",
        "    expected_output=\"A document free of gender or socioeconomic bias\"\n",
        ")\n",
        "\n",
        "# Define the metric\n",
        "bias_metric = BiasMetric()\n",
        "\n",
        "# Run evaluation\n",
        "evaluate([test_case], [bias_metric])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "58WZgtU_Rufb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compliance CheckList"
      ],
      "metadata": {
        "id": "UsOyKXgCuX4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
        "from deepeval.metrics.dag import (\n",
        "    DeepAcyclicGraph,\n",
        "    TaskNode,\n",
        "    BinaryJudgementNode,\n",
        "    VerdictNode,\n",
        ")"
      ],
      "metadata": {
        "id": "IGYUYh99d5iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load your protocol from .docx\n",
        "def load_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])"
      ],
      "metadata": {
        "id": "s3pShwvP_7Iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protocol_text = load_docx(\"protocolv1.docx\")\n",
        "\n",
        "# Step 2: Create the test case\n",
        "test_case = LLMTestCase(\n",
        "    input=\"Check if the clinical trial includes the specified sections\",\n",
        "    actual_output=protocol_text,\n",
        ")\n",
        "\n",
        "def make_binary_check(criteria_text):\n",
        "    return BinaryJudgementNode(\n",
        "        criteria=criteria_text,\n",
        "        children=[\n",
        "            VerdictNode(verdict=False, score=0),\n",
        "            VerdictNode(verdict=True, score=1),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "compliance_checks = [\n",
        "    make_binary_check(\"Does the protocol include the trial phase?\"),\n",
        "    make_binary_check(\"Does the protocol include a study design section?\"),\n",
        "    make_binary_check(\"Does the protocol include a primary objective section?\"),\n",
        "    make_binary_check(\"Does the protocol include a secondary objective section?\"),\n",
        "    make_binary_check(\"Does the protocol include a primary endpoint section?\"),\n",
        "    make_binary_check(\"Does the protocol include a secondary endpoint section?\"),\n",
        "    make_binary_check(\"Does the protocol include an inclusion criteria section?\"),\n",
        "    make_binary_check(\"Does the protocol include an exclusion criteria section?\"),\n",
        "    make_binary_check(\"Does the protocol include a statistical considerations section?\"),\n",
        "]\n",
        "\n",
        "compliance_task_node = TaskNode(\n",
        "    instructions=\"Check if the clinical trial includes the specified sections\",\n",
        "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
        "    output_label=\"Protocol Content\",\n",
        "    children=compliance_checks,\n",
        ")\n",
        "\n",
        "dag = DeepAcyclicGraph(root_nodes=[compliance_task_node])\n"
      ],
      "metadata": {
        "id": "h1Yxyl138iUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval.metrics import DAGMetric\n",
        "\n",
        "format_correctness = DAGMetric(name=\"Contains Section\", dag=dag, include_reason=True, verbose_mode=True)\n",
        "format_correctness.measure(test_case)\n",
        "print(format_correctness.score)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m-BRkvJRvsRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization"
      ],
      "metadata": {
        "id": "HSe9QTQKdltX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "background_summary = \"\"\"\n",
        "This Phase 2 clinical trial evaluates Ilizomab, a novel monoclonal antibody,\n",
        "in adult patients with moderate to severe Systemic Lupus Erythematosus (SLE).\n",
        "The study is a randomized, double-blind, placebo-controlled trial involving\n",
        "approximately 150 participants, aiming to assess the safety, efficacy, and\n",
        "pharmacokinetics of Ilizomab. Ilizomab targets a specific immune pathway\n",
        "implicated in lupus, showing promise in preclinical studies by modulating\n",
        "inflammatory cytokines and reducing autoantibody production. The trial spans\n",
        "24 weeks of treatment followed by 12 weeks of follow-up, with primary outcomes\n",
        "measured at Week 24 using the SLE Responder Index (SRI-4). Secondary objectives\n",
        "include evaluating changes in disease activity, biomarker levels, and\n",
        "patient-reported outcomes. Safety, immunogenicity, and adverse event rates\n",
        "will be closely monitored throughout the study.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load and clean text from docx\n",
        "def read_docx(path):\n",
        "    doc = Document(path)\n",
        "    return \"\\n\".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "# Load the full protocol as source\n",
        "source_text = read_docx(\"protocolv1.docx\")"
      ],
      "metadata": {
        "id": "lwnE13J9h-gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics import SummarizationMetric\n",
        "\n",
        "# Create a DeepEval test case for the purposes of the evaluation\n",
        "test_case = LLMTestCase(\n",
        "  input = source_text,\n",
        "  actual_output = background_summary\n",
        ")\n",
        "\n",
        "# Instantiate the summarization metric\n",
        "summarization_metric = SummarizationMetric(verbose_mode = True, n = 20, truths_extraction_limit = 20)\n",
        "\n",
        "# Run the evaluation on the test case\n",
        "eval_result = evaluate([test_case], [summarization_metric])"
      ],
      "metadata": {
        "id": "GYYuuhx7deQE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}